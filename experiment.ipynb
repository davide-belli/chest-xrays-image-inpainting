{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from model import _netG\n",
    "\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--dataset DATASET]\n",
      "                             [--test_image TEST_IMAGE] [--workers WORKERS]\n",
      "                             [--batchSize BATCHSIZE] [--imageSize IMAGESIZE]\n",
      "                             [--nz NZ] [--ngf NGF] [--ndf NDF] [--nc NC]\n",
      "                             [--niter NITER] [--lr LR] [--beta1 BETA1]\n",
      "                             [--cuda] [--ngpu NGPU] [--netG NETG]\n",
      "                             [--netD NETD] [--outf OUTF]\n",
      "                             [--manualSeed MANUALSEED]\n",
      "                             [--nBottleneck NBOTTLENECK]\n",
      "                             [--overlapPred OVERLAPPRED] [--nef NEF]\n",
      "                             [--wtl2 WTL2]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f /run/user/1000/jupyter/kernel-e0388a81-8cce-4c2e-987c-fc216eeacbc9.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/davide/miniconda3/envs/honours-pytorch/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2870: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--dataset',  default='streetview', help='cifar10 | lsun | imagenet | folder | lfw ')\n",
    "parser.add_argument('--test_image', default='lungs.png', help='path to dataset')\n",
    "parser.add_argument('--workers', type=int, help='number of data loading workers', default=4)\n",
    "parser.add_argument('--batchSize', type=int, default=64, help='input batch size')\n",
    "parser.add_argument('--imageSize', type=int, default=128, help='the height / width of the input image to network')\n",
    "parser.add_argument('--nz', type=int, default=100, help='size of the latent z vector')\n",
    "parser.add_argument('--ngf', type=int, default=64)\n",
    "parser.add_argument('--ndf', type=int, default=64)\n",
    "parser.add_argument('--nc', type=int, default=3)\n",
    "parser.add_argument('--niter', type=int, default=25, help='number of epochs to train for')\n",
    "parser.add_argument('--lr', type=float, default=0.0002, help='learning rate, default=0.0002')\n",
    "parser.add_argument('--beta1', type=float, default=0.5, help='beta1 for adam. default=0.5')\n",
    "parser.add_argument('--cuda', action='store_true', help='enables cuda')\n",
    "parser.add_argument('--ngpu', type=int, default=1, help='number of GPUs to use')\n",
    "parser.add_argument('--netG', default='model/netG_streetview.pth', help=\"path to netG (to continue training)\")\n",
    "parser.add_argument('--netD', default='', help=\"path to netD (to continue training)\")\n",
    "parser.add_argument('--outf', default='.', help='folder to output images and model checkpoints')\n",
    "parser.add_argument('--manualSeed', type=int, help='manual seed')\n",
    "\n",
    "parser.add_argument('--nBottleneck', type=int,default=4000,help='of dim for bottleneck of encoder')\n",
    "parser.add_argument('--overlapPred',type=int,default=4,help='overlapping edges')\n",
    "parser.add_argument('--nef',type=int,default=64,help='of encoder filters in first conv layer')\n",
    "parser.add_argument('--wtl2',type=float,default=0.999,help='0 means do not use else use with this weight')\n",
    "opt = parser.parse_args()\n",
    "# print(opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'opt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-ff0f5f7b9a7a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnetG\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_netG\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# netG = TransformerNet()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mnetG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetG\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstorage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'state_dict'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# netG.requires_grad = False\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mnetG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'opt' is not defined"
     ]
    }
   ],
   "source": [
    "netG = _netG(opt)\n",
    "# netG = TransformerNet()\n",
    "netG.load_state_dict(torch.load(opt.netG,map_location=lambda storage, location: storage)['state_dict'])\n",
    "# netG.requires_grad = False\n",
    "netG.eval()\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "\n",
    "image = utils.load_image(opt.test_image, opt.imageSize)\n",
    "image = transform(image)\n",
    "image = image.repeat(1, 1, 1, 1)\n",
    "\n",
    "input_real = torch.FloatTensor(1, 3, opt.imageSize, opt.imageSize)\n",
    "input_cropped = torch.FloatTensor(1, 3, opt.imageSize, opt.imageSize)\n",
    "real_center = torch.FloatTensor(1, 3, int(opt.imageSize/2), int(opt.imageSize/2))\n",
    "\n",
    "criterionMSE = nn.MSELoss()\n",
    "\n",
    "# if opt.cuda:\n",
    "#     netG.cuda()\n",
    "#     input_real, input_cropped = input_real.cuda(),input_cropped.cuda()\n",
    "#     criterionMSE.cuda()\n",
    "#     real_center = real_center.cuda()\n",
    "\n",
    "input_real = Variable(input_real)\n",
    "input_cropped = Variable(input_cropped)\n",
    "real_center = Variable(real_center)\n",
    "\n",
    "\n",
    "input_real.data.resize_(image.size()).copy_(image)\n",
    "input_cropped.data.resize_(image.size()).copy_(image)\n",
    "real_center_cpu = image[:,:,int(opt.imageSize/4):int(opt.imageSize/4+opt.imageSize/2),int(opt.imageSize/4):int(opt.imageSize/4+opt.imageSize/2)]\n",
    "real_center.data.resize_(real_center_cpu.size()).copy_(real_center_cpu)\n",
    "\n",
    "input_cropped.data[:,0,int(int(opt.imageSize/4)+opt.overlapPred):int(int(opt.imageSize/4)+int(opt.imageSize/2)-opt.overlapPred),int(int(opt.imageSize/4)+opt.overlapPred):int(int(opt.imageSize/4)+int(opt.imageSize/2)-opt.overlapPred)] = 2*117.0/255.0 - 1.0\n",
    "input_cropped.data[:,1,int(int(opt.imageSize/4)+opt.overlapPred):int(int(opt.imageSize/4)+int(opt.imageSize/2)-opt.overlapPred),int(int(opt.imageSize/4)+opt.overlapPred):int(int(opt.imageSize/4)+int(opt.imageSize/2)-opt.overlapPred)] = 2*104.0/255.0 - 1.0\n",
    "input_cropped.data[:,2,int(int(opt.imageSize/4)+opt.overlapPred):int(int(opt.imageSize/4)+int(opt.imageSize/2)-opt.overlapPred),int(int(opt.imageSize/4)+opt.overlapPred):int(int(opt.imageSize/4)+int(opt.imageSize/2)-opt.overlapPred)] = 2*123.0/255.0 - 1.0\n",
    "\n",
    "fake = netG(input_cropped)\n",
    "errG = criterionMSE(fake,real_center)\n",
    "\n",
    "recon_image = input_cropped.clone()\n",
    "recon_image.data[:,:,int(opt.imageSize/4):int(opt.imageSize/4)+int(opt.imageSize/2),int(opt.imageSize/4):int(opt.imageSize/4)+int(opt.imageSize/2)] = fake.data\n",
    "\n",
    "utils.save_image('val_real_samples.png',image[0])\n",
    "utils.save_image('val_cropped_samples.png',input_cropped.data[0])\n",
    "utils.save_image('val_recon_samples.png',recon_image.data[0])\n",
    "\n",
    "print('%.4f' % errG.data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
